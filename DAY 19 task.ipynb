{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9968880b-9aa4-4b96-9fb3-96f7e32f60e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA Model trained successfully!\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                  PM2.5   No. Observations:                 6324\n",
      "Model:                 ARIMA(2, 1, 2)   Log Likelihood              -31883.228\n",
      "Date:                Fri, 17 Oct 2025   AIC                          63776.455\n",
      "Time:                        18:52:17   BIC                          63810.215\n",
      "Sample:                             0   HQIC                         63788.148\n",
      "                               - 6324                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "ar.L1         -0.5331      0.135     -3.940      0.000      -0.798      -0.268\n",
      "ar.L2         -0.0347      0.009     -3.774      0.000      -0.053      -0.017\n",
      "ma.L1         -0.4095      0.136     -3.017      0.003      -0.675      -0.144\n",
      "ma.L2         -0.5033      0.129     -3.912      0.000      -0.756      -0.251\n",
      "sigma2      1403.3218      6.814    205.942      0.000    1389.966    1416.677\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):            188933.01\n",
      "Prob(Q):                              0.97   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               5.27   Skew:                             3.37\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                        28.91\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  ARIMA Model \n",
    "\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#  Load the Dataset\n",
    "df = pd.read_csv(\"station_day_edited_with_missing.csv\")\n",
    "\n",
    "# Basic Preprocessing\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Drop rows with missing values in any column \n",
    "df = df.dropna()\n",
    "\n",
    "# Set Date as index (required for ARIMA)\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "#  Target Variable Selection\n",
    "target_col = 'PM2.5' if 'PM2.5' in df.columns else df.columns[-1]\n",
    "\n",
    "# Create Lag Features for better temporal learning\n",
    "df['lag_1'] = df[target_col].shift(1)\n",
    "df['lag_2'] = df[target_col].shift(2)\n",
    "df['lag_3'] = df[target_col].shift(3)\n",
    "\n",
    "# Drop any rows with NaN created due to lagging\n",
    "df = df.dropna()\n",
    "\n",
    "# Split into Train Set (80%)\n",
    "split_index = int(len(df) * 0.8)\n",
    "train = df.iloc[:split_index]\n",
    "\n",
    "#  Build & Train ARIMA Model\n",
    "# (p, d, q) parameters can be tuned later\n",
    "p, d, q = 2, 1, 2\n",
    "\n",
    "model = ARIMA(train[target_col], order=(p, d, q))\n",
    "model_fit = model.fit()\n",
    "\n",
    "print(\"ARIMA Model trained successfully!\")\n",
    "print(model_fit.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913dd075-fc41-4a88-99f6-330d1d0501ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:52:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:53:09 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prophet model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  PROPHET Model (with lag features)\n",
    "\n",
    "#  Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2️⃣ Load the Dataset\n",
    "df = pd.read_csv(\"station_day_edited_with_missing.csv\")\n",
    "\n",
    "# 3️⃣ Basic Preprocessing\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Select target variable (example: PM2.5)\n",
    "target_col = 'PM2.5' if 'PM2.5' in df.columns else df.columns[-1]\n",
    "\n",
    "# Fill missing numeric values\n",
    "df[target_col] = df[target_col].fillna(df[target_col].mean())\n",
    "\n",
    "# 4️⃣ Create Lag Features\n",
    "df['lag_1'] = df[target_col].shift(1)\n",
    "df['lag_2'] = df[target_col].shift(2)\n",
    "df['lag_3'] = df[target_col].shift(3)\n",
    "\n",
    "# Drop missing rows from lagging\n",
    "df = df.dropna()\n",
    "\n",
    "# 5️⃣ Prepare Data for Prophet\n",
    "# Prophet requires columns: ds (date) and y (target)\n",
    "prophet_df = df[['Date', target_col, 'lag_1', 'lag_2', 'lag_3']].rename(columns={'Date': 'ds', target_col: 'y'})\n",
    "\n",
    "# 6️⃣ Train–Test Split (we only train)\n",
    "split_index = int(len(prophet_df) * 0.8)\n",
    "train_df = prophet_df.iloc[:split_index]\n",
    "\n",
    "# 7️⃣ Build Prophet Model\n",
    "model = Prophet()\n",
    "\n",
    "# Add lag features as external regressors\n",
    "model.add_regressor('lag_1')\n",
    "model.add_regressor('lag_2')\n",
    "model.add_regressor('lag_3')\n",
    "\n",
    "# 8️⃣ Fit (Train) the Model\n",
    "model.fit(train_df)\n",
    "\n",
    "print(\"✅ Prophet model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c54072-b684-427d-be15-07b5af81f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0034\n",
      "Epoch 2/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0032\n",
      "Epoch 3/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0032\n",
      "Epoch 4/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0032\n",
      "Epoch 5/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0032\n",
      "Epoch 6/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0032\n",
      "Epoch 7/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 8/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 9/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 10/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 11/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 12/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 13/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 14/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 15/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 16/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 17/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 18/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 19/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 20/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 21/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 22/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 23/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 24/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 25/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 26/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 27/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 28/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 29/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 30/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 31/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 32/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 33/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 34/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 35/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 36/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 37/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 38/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 39/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 40/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 41/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 42/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 43/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 44/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 45/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 46/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 47/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031\n",
      "Epoch 48/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 49/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 50/50\n",
      "\u001b[1m402/402\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031\n",
      "LSTM model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  LSTM Model\n",
    "\n",
    "#  Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the Dataset\n",
    "df = pd.read_csv(\"station_day_edited_with_missing.csv\")\n",
    "\n",
    "#  Basic Preprocessing\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Choose target variable \n",
    "target_col = 'PM2.5' if 'PM2.5' in df.columns else df.columns[-1]\n",
    "\n",
    "# Fill missing numeric values\n",
    "df[target_col] = df[target_col].fillna(df[target_col].mean())\n",
    "\n",
    "# Create Lag Features (to help LSTM learn time dependencies)\n",
    "df['lag_1'] = df[target_col].shift(1)\n",
    "df['lag_2'] = df[target_col].shift(2)\n",
    "df['lag_3'] = df[target_col].shift(3)\n",
    "\n",
    "# Drop rows with missing lag values\n",
    "df = df.dropna()\n",
    "\n",
    "# Feature Scaling (LSTM works best with scaled data)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df[[target_col, 'lag_1', 'lag_2', 'lag_3']])\n",
    "\n",
    "# Convert back to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=[target_col, 'lag_1', 'lag_2', 'lag_3'])\n",
    "scaled_df.index = df['Date']\n",
    "\n",
    "#  Prepare Features (X) and Target (y)\n",
    "X = scaled_df[['lag_1', 'lag_2', 'lag_3']].values\n",
    "y = scaled_df[target_col].values\n",
    "\n",
    "# Reshape input to [samples, timesteps, features] for LSTM\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Split into Training Data (first 80%)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, y_train = X[:split_index], y[:split_index]\n",
    "\n",
    "# Build LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "print(\"LSTM model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "157ea862-8f95-44cb-98c0-10dc382eb8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# XGBoost Model (with lag features)\n",
    "\n",
    "\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#  Load the Dataset\n",
    "df = pd.read_csv(\"station_day_edited_with_missing.csv\")\n",
    "\n",
    "# Basic Preprocessing\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Choose target variable (example: PM2.5)\n",
    "target_col = 'PM2.5' if 'PM2.5' in df.columns else df.columns[-1]\n",
    "\n",
    "# Fill missing numeric values\n",
    "df[target_col] = df[target_col].fillna(df[target_col].mean())\n",
    "\n",
    "#  Create Lag Features\n",
    "# These represent past observations as features\n",
    "df['lag_1'] = df[target_col].shift(1)\n",
    "df['lag_2'] = df[target_col].shift(2)\n",
    "df['lag_3'] = df[target_col].shift(3)\n",
    "df['lag_4'] = df[target_col].shift(4)\n",
    "df['lag_5'] = df[target_col].shift(5)\n",
    "\n",
    "# Drop rows with missing lag values\n",
    "df = df.dropna()\n",
    "\n",
    "# Feature Scaling (optional but helps XGBoost learn faster)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df[[target_col, 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5']])\n",
    "\n",
    "scaled_df = pd.DataFrame(\n",
    "    scaled_data, \n",
    "    columns=[target_col, 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5']\n",
    ")\n",
    "scaled_df.index = df['Date']\n",
    "\n",
    "#  Prepare Input and Output\n",
    "X = scaled_df[['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5']]\n",
    "y = scaled_df[target_col]\n",
    "\n",
    "#  Split Data into Training (80%) and Testing (20%) \n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, y_train = X.iloc[:split_index], y.iloc[:split_index]\n",
    "\n",
    "# Build XGBoost Model\n",
    "model = XGBRegressor(\n",
    "    n_estimators=200,       \n",
    "    learning_rate=0.05,   \n",
    "    max_depth=5,           \n",
    "    subsample=0.8,         \n",
    "    colsample_bytree=0.8,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"XGBoost model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b148ab-69e3-4326-a3ef-1b9f8b0d3280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
