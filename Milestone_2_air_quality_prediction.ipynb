{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f97911-f26a-4de2-82ab-0b796fbaa937",
   "metadata": {},
   "source": [
    "# Milestone 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07623f8-de53-440a-8046-abfbb490fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV: air_quality_data.csv  shape: (29531, 16)\n",
      "Columns: ['City', 'Date', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket']\n",
      "Detected datetime column: Date\n",
      "Detected group column: City\n",
      "Selected target column: PM2.5\n",
      "\n",
      "--- EDA summary for target ---\n",
      "              PM2.5\n",
      "count  24933.000000\n",
      "mean      67.450578\n",
      "std       64.661449\n",
      "min        0.040000\n",
      "1%         7.853200\n",
      "5%        13.206000\n",
      "25%       28.820000\n",
      "50%       48.570000\n",
      "75%       80.590000\n",
      "95%      193.960000\n",
      "99%      311.064000\n",
      "max      949.990000\n",
      "\n",
      "Null counts per column:\n",
      "City              0\n",
      "PM2.5          4598\n",
      "PM10          11140\n",
      "NO             3582\n",
      "NO2            3585\n",
      "NOx            4185\n",
      "NH3           10328\n",
      "CO             2059\n",
      "SO2            3854\n",
      "O3             4022\n",
      "Benzene        5623\n",
      "Toluene        8041\n",
      "Xylene        18109\n",
      "AQI            4681\n",
      "AQI_Bucket     4681\n",
      "dtype: int64\n",
      "Resample skipped: agg function failed [how->mean,dtype->object]\n",
      "Saved cleaned dataset to: ./artifacts\\air_quality_data_cleaned.csv\n",
      "Groups detected: ['Ahmedabad', 'Chennai', 'Delhi', 'Lucknow', 'Mumbai', 'Bengaluru', 'Hyderabad', 'Patna', 'Gurugram', 'Visakhapatnam']\n",
      "Libraries available: pmdarima: False statsmodels: True prophet: True xgboost: True tensorflow: True joblib: True\n",
      "\n",
      "Group Ahmedabad: train=1607 test=402\n",
      "Fitting SARIMAX(1,1,1) fallback...\n",
      "SARIMAX done: 17.219941167701435 22.712714998901355\n",
      "Fitting Prophet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:43:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:38 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet done: 25.080491605281534 31.2894550921331\n",
      "Training XGBoost...\n",
      "XGBoost done: 15.130029682435298 21.99320316188497\n",
      "TensorFlow version: 2.20.0\n",
      "Training LSTM (simple)...\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 657ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM done: 30.10061264038086 32.601863861083984\n",
      "Best model for group Ahmedabad => XGBoost (MAE=15.1300, RMSE=21.9932) path=./artifacts\\xgboost_Ahmedabad.pkl\n",
      "\n",
      "Group Chennai: train=1607 test=402\n",
      "Fitting SARIMAX(1,1,1) fallback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:cmdstanpy:cmd: where.exe tbb.dll\n",
      "cwd: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARIMAX done: 16.08903945557664 21.1850380484624\n",
      "Fitting Prophet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:cmdstanpy:TBB already found in load path\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: C:\\Users\\vsr\\AppData\\Local\\Temp\\tmp0tnv5wz2\\3ogfnyzg.json\n",
      "DEBUG:cmdstanpy:input tempfile: C:\\Users\\vsr\\AppData\\Local\\Temp\\tmp0tnv5wz2\\snae0r9h.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\prophet\\\\stan_model\\\\prophet_model.bin', 'random', 'seed=5488', 'data', 'file=C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Temp\\\\tmp0tnv5wz2\\\\3ogfnyzg.json', 'init=C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Temp\\\\tmp0tnv5wz2\\\\snae0r9h.json', 'output', 'file=C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Temp\\\\tmp0tnv5wz2\\\\prophet_model9fn0zryf\\\\prophet_model-20251119174916.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
      "17:49:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "17:49:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet done: 17.93166162529626 22.806552192556\n",
      "Training XGBoost...\n",
      "XGBoost done: 13.486693427069229 20.1701014024599\n",
      "TensorFlow version: 2.20.0\n",
      "Training LSTM (simple)...\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 394ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM done: 9.921530723571777 15.267081260681152\n",
      "Best model for group Chennai => LSTM (MAE=9.9215, RMSE=15.2671) path=./artifacts\\lstm_Chennai.h5\n",
      "\n",
      "Group Delhi: train=1607 test=402\n",
      "Fitting SARIMAX(1,1,1) fallback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:cmdstanpy:cmd: where.exe tbb.dll\n",
      "cwd: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARIMAX done: 52.56070583446492 81.96043918550596\n",
      "Fitting Prophet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:cmdstanpy:TBB already found in load path\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: C:\\Users\\vsr\\AppData\\Local\\Temp\\tmp0tnv5wz2\\6hk1zxxv.json\n",
      "DEBUG:cmdstanpy:input tempfile: C:\\Users\\vsr\\AppData\\Local\\Temp\\tmp0tnv5wz2\\xpbf92yu.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\prophet\\\\stan_model\\\\prophet_model.bin', 'random', 'seed=72426', 'data', 'file=C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Temp\\\\tmp0tnv5wz2\\\\6hk1zxxv.json', 'init=C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Temp\\\\tmp0tnv5wz2\\\\xpbf92yu.json', 'output', 'file=C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Temp\\\\tmp0tnv5wz2\\\\prophet_modeltw1ri6pm\\\\prophet_model-20251119175128.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
      "17:51:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "17:51:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet done: 34.84522031937735 49.64072388767631\n",
      "Training XGBoost...\n",
      "XGBoost done: 26.264461983088545 44.24633510177533\n",
      "TensorFlow version: 2.20.0\n",
      "Training LSTM (simple)...\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM done: 24.242023468017578 40.029808044433594\n",
      "Best model for group Delhi => LSTM (MAE=24.2420, RMSE=40.0298) path=./artifacts\\lstm_Delhi.h5\n",
      "\n",
      "Group Lucknow: train=1607 test=402\n",
      "Fitting SARIMAX(1,1,1) fallback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:cmdstanpy:cmd: where.exe tbb.dll\n",
      "cwd: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARIMAX done: 42.289696455234484 56.21156010850941\n",
      "Fitting Prophet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:cmdstanpy:TBB already found in load path\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: C:\\Users\\vsr\\AppData\\Local\\Temp\\tmp0tnv5wz2\\de_v5i5p.json\n",
      "DEBUG:cmdstanpy:input tempfile: C:\\Users\\vsr\\AppData\\Local\\Temp\\tmp0tnv5wz2\\sxqtrx49.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\prophet\\\\stan_model\\\\prophet_model.bin', 'random', 'seed=31906', 'data', 'file=C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Temp\\\\tmp0tnv5wz2\\\\de_v5i5p.json', 'init=C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Temp\\\\tmp0tnv5wz2\\\\sxqtrx49.json', 'output', 'file=C:\\\\Users\\\\vsr\\\\AppData\\\\Local\\\\Temp\\\\tmp0tnv5wz2\\\\prophet_modelpema8zfj\\\\prophet_model-20251119175239.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
      "17:52:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "air_quality_pipeline.py\n",
    "\n",
    "Pipeline:\n",
    "1. Imports\n",
    "2. Data cleaning + EDA (saves cleaned CSV)\n",
    "3. Train/test split (time-based)\n",
    "4. Train ARIMA/Prophet/XGBoost/LSTM (if packages available)\n",
    "5. Evaluate MAE & RMSE and pick best model per group\n",
    "6. Save best model(s) for inference (joblib / model.save)\n",
    "7. Handles per-city / per-station models if a grouping column is found\n",
    "\"\"\"\n",
    "\n",
    "import os, math, traceback, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# -------- Config --------\n",
    "DATA_PATH = \"air_quality_data.csv\"   # change if needed\n",
    "OUT_DIR = \"./artifacts\"\n",
    "CLEANED_PATH = os.path.join(OUT_DIR, \"air_quality_data_cleaned.csv\")\n",
    "BEST_MODELS_PATH = os.path.join(OUT_DIR, \"best_models_per_group.csv\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------- Utilities --------\n",
    "def mae(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "\n",
    "def time_train_test_split(series, test_size=0.2):\n",
    "    n = len(series)\n",
    "    split = int(n * (1 - test_size))\n",
    "    train = series.iloc[:split]\n",
    "    test = series.iloc[split:]\n",
    "    return train, test\n",
    "\n",
    "# -------- Load data --------\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {DATA_PATH}. Update DATA_PATH accordingly.\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded CSV:\", DATA_PATH, \" shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# -------- Detect datetime column --------\n",
    "datetime_col = None\n",
    "for c in df.columns:\n",
    "    if any(k in c.lower() for k in (\"date\",\"time\",\"timestamp\")):\n",
    "        datetime_col = c\n",
    "        break\n",
    "if datetime_col is None:\n",
    "    # try to find object column that parses to datetime\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            sample = df[c].dropna().astype(str).head(20).tolist()\n",
    "            ok = True\n",
    "            for s in sample:\n",
    "                try:\n",
    "                    pd.to_datetime(s)\n",
    "                except:\n",
    "                    ok = False\n",
    "                    break\n",
    "            if ok:\n",
    "                datetime_col = c\n",
    "                break\n",
    "print(\"Detected datetime column:\", datetime_col)\n",
    "\n",
    "if datetime_col:\n",
    "    df[datetime_col] = pd.to_datetime(df[datetime_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[datetime_col]).sort_values(by=datetime_col).reset_index(drop=True)\n",
    "    df = df.set_index(datetime_col)\n",
    "else:\n",
    "    print(\"No datetime column detected — will use integer index. Time-series models expect a time index for best results.\")\n",
    "\n",
    "# -------- Detect group column (city/station) --------\n",
    "group_col = None\n",
    "for c in df.columns:\n",
    "    if any(k in c.lower() for k in (\"city\",\"station\",\"location\",\"site\")):\n",
    "        group_col = c\n",
    "        break\n",
    "print(\"Detected group column:\", group_col)\n",
    "\n",
    "# -------- Detect target pollutant column --------\n",
    "poll_candidates = [c for c in df.columns if any(p in c.lower() for p in (\"pm2.5\",\"pm25\",\"pm10\",\"pm 2.5\",\"pm_10\",\"pm_25\",\"pm\"))]\n",
    "if not poll_candidates:\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    poll_candidates = [c for c in numeric_cols if c != group_col]\n",
    "if not poll_candidates:\n",
    "    raise ValueError(\"No numeric pollutant-like columns found.\")\n",
    "target = max(poll_candidates, key=lambda c: df[c].notna().sum())\n",
    "print(\"Selected target column:\", target)\n",
    "\n",
    "# -------- EDA summary (minimal) --------\n",
    "print(\"\\n--- EDA summary for target ---\")\n",
    "print(df[[target]].describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99]))\n",
    "print(\"\\nNull counts per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# -------- Cleaning --------\n",
    "cleaned = df.copy()\n",
    "if isinstance(cleaned.index, pd.DatetimeIndex):\n",
    "    # if high freq (sub-daily) -> resample to daily mean; else set daily freq\n",
    "    try:\n",
    "        delta = cleaned.index.to_series().diff().median()\n",
    "        if pd.notnull(delta) and delta < pd.Timedelta(days=1):\n",
    "            cleaned = cleaned.resample('D').mean()\n",
    "            print(\"Resampled to daily mean.\")\n",
    "        else:\n",
    "            cleaned = cleaned.asfreq('D')\n",
    "            print(\"Set index frequency to daily (may introduce NaNs).\")\n",
    "    except Exception as e:\n",
    "        print(\"Resample skipped:\", e)\n",
    "num_cols = cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if isinstance(cleaned.index, pd.DatetimeIndex):\n",
    "    cleaned[num_cols] = cleaned[num_cols].interpolate(method='time', limit_direction='both')\n",
    "else:\n",
    "    cleaned[num_cols] = cleaned[num_cols].interpolate().ffill().bfill()\n",
    "cleaned = cleaned.dropna(subset=[target])\n",
    "cleaned.to_csv(CLEANED_PATH)\n",
    "print(\"Saved cleaned dataset to:\", CLEANED_PATH)\n",
    "\n",
    "# -------- Setup groups --------\n",
    "if group_col and group_col in cleaned.columns:\n",
    "    groups = cleaned[group_col].dropna().unique().tolist()\n",
    "    if len(groups) == 0:\n",
    "        groups = [None]\n",
    "else:\n",
    "    groups = [None]\n",
    "print(\"Groups detected:\", groups[:10])\n",
    "\n",
    "# -------- Optional libs detection (import only if available) --------\n",
    "_has_joblib = True\n",
    "try:\n",
    "    import joblib\n",
    "except:\n",
    "    _has_joblib = False\n",
    "\n",
    "def safe_import(name):\n",
    "    try:\n",
    "        module = __import__(name)\n",
    "        return module\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "pmd = safe_import(\"pmdarima\")\n",
    "statsmodels = safe_import(\"statsmodels\")\n",
    "prophet_mod = safe_import(\"prophet\") or safe_import(\"fbprophet\")\n",
    "xgb_mod = safe_import(\"xgboost\")\n",
    "tf_mod = safe_import(\"tensorflow\")\n",
    "\n",
    "print(\"Libraries available: pmdarima:\", bool(pmd), \"statsmodels:\", bool(statsmodels),\n",
    "      \"prophet:\", bool(prophet_mod), \"xgboost:\", bool(xgb_mod), \"tensorflow:\", bool(tf_mod), \"joblib:\", _has_joblib)\n",
    "\n",
    "# -------- Modeling loop per group --------\n",
    "results = []\n",
    "for grp in groups:\n",
    "    try:\n",
    "        if grp is None:\n",
    "            s_df = cleaned[[target]].dropna().copy()\n",
    "            gname = \"global\"\n",
    "        else:\n",
    "            s_df = cleaned[cleaned[group_col] == grp][[target]].dropna().copy()\n",
    "            gname = str(grp)\n",
    "        if len(s_df) < 10:\n",
    "            print(f\"Skipping group {gname}: insufficient data ({len(s_df)} rows).\")\n",
    "            continue\n",
    "        if not isinstance(s_df.index, pd.DatetimeIndex):\n",
    "            s_df.index = pd.date_range(start=\"2000-01-01\", periods=len(s_df), freq='D')\n",
    "        train, test = time_train_test_split(s_df[target], test_size=0.2)\n",
    "        print(f\"\\nGroup {gname}: train={len(train)} test={len(test)}\")\n",
    "        model_scores = {}\n",
    "\n",
    "        # --- ARIMA (pmdarima preferred) ---\n",
    "        try:\n",
    "            if pmd is not None:\n",
    "                print(\"Fitting pmdarima auto_arima...\")\n",
    "                ar = pmd.arima.auto_arima(train, seasonal=False, error_action='ignore', suppress_warnings=True)\n",
    "                preds = ar.predict(n_periods=len(test))\n",
    "                m_mae, m_rmse = mae(test.values, preds), rmse(test.values, preds)\n",
    "                ar_path = os.path.join(OUT_DIR, f\"arima_{gname}.pkl\")\n",
    "                if _has_joblib:\n",
    "                    joblib.dump(ar, ar_path)\n",
    "                model_scores[\"ARIMA\"] = (m_mae, m_rmse, ar_path)\n",
    "                print(\"ARIMA done:\", m_mae, m_rmse)\n",
    "            elif statsmodels is not None:\n",
    "                import statsmodels.api as sm\n",
    "                from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "                print(\"Fitting SARIMAX(1,1,1) fallback...\")\n",
    "                m = SARIMAX(train, order=(1,1,1), enforce_stationarity=False, enforce_invertibility=False)\n",
    "                res = m.fit(disp=False)\n",
    "                preds = res.get_forecast(steps=len(test)).predicted_mean\n",
    "                m_mae, m_rmse = mae(test.values, preds.values), rmse(test.values, preds.values)\n",
    "                ar_path = os.path.join(OUT_DIR, f\"sarimax_{gname}.pkl\")\n",
    "                if _has_joblib:\n",
    "                    joblib.dump(res, ar_path)\n",
    "                model_scores[\"ARIMA\"] = (m_mae, m_rmse, ar_path)\n",
    "                print(\"SARIMAX done:\", m_mae, m_rmse)\n",
    "            else:\n",
    "                print(\"ARIMA skipped (no pmdarima or statsmodels).\")\n",
    "        except Exception as e:\n",
    "            print(\"ARIMA error:\", e)\n",
    "\n",
    "        # --- Prophet ---\n",
    "        try:\n",
    "            if prophet_mod is not None:\n",
    "                print(\"Fitting Prophet...\")\n",
    "                from prophet import Prophet as ProphetClass\n",
    "                ptrain = train.reset_index().rename(columns={train.index.name or \"index\":\"ds\", train.name:\"y\"})\n",
    "                mp = ProphetClass()\n",
    "                mp.fit(ptrain)\n",
    "                future = mp.make_future_dataframe(periods=len(test), freq='D')\n",
    "                fc = mp.predict(future)\n",
    "                preds = fc['yhat'].iloc[-len(test):].values\n",
    "                p_mae, p_rmse = mae(test.values, preds), rmse(test.values, preds)\n",
    "                p_path = os.path.join(OUT_DIR, f\"prophet_{gname}.pkl\")\n",
    "                if _has_joblib:\n",
    "                    joblib.dump(mp, p_path)\n",
    "                model_scores[\"Prophet\"] = (p_mae, p_rmse, p_path)\n",
    "                print(\"Prophet done:\", p_mae, p_rmse)\n",
    "            else:\n",
    "                print(\"Prophet skipped.\")\n",
    "        except Exception as e:\n",
    "            print(\"Prophet error:\", e)\n",
    "\n",
    "        # --- XGBoost with lag features ---\n",
    "        try:\n",
    "            if xgb_mod is not None:\n",
    "                print(\"Training XGBoost...\")\n",
    "                df_feat = s_df.copy()\n",
    "                for lag in range(1, 8):\n",
    "                    df_feat[f\"lag_{lag}\"] = df_feat[target].shift(lag)\n",
    "                df_feat = df_feat.dropna()\n",
    "                X = df_feat[[c for c in df_feat.columns if c.startswith(\"lag_\")]].values\n",
    "                y = df_feat[target].values\n",
    "                split_idx = int(len(X) * 0.8)\n",
    "                X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "                y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "                model_xgb = xgb_mod.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "                model_xgb.fit(X_train, y_train)\n",
    "                preds = model_xgb.predict(X_test)\n",
    "                x_mae, x_rmse = mae(y_test, preds), rmse(y_test, preds)\n",
    "                x_path = os.path.join(OUT_DIR, f\"xgboost_{gname}.pkl\")\n",
    "                if _has_joblib:\n",
    "                    joblib.dump(model_xgb, x_path)\n",
    "                model_scores[\"XGBoost\"] = (x_mae, x_rmse, x_path)\n",
    "                print(\"XGBoost done:\", x_mae, x_rmse)\n",
    "            else:\n",
    "                print(\"XGBoost skipped.\")\n",
    "        except Exception as e:\n",
    "            print(\"XGBoost error:\", e)\n",
    "\n",
    "        # --- LSTM ---\n",
    "        try:\n",
    "            import tensorflow as tf_mod\n",
    "            print(\"TensorFlow version:\", tf_mod.__version__)\n",
    "        except ImportError:\n",
    "            tf_mod = None\n",
    "            print(\"TensorFlow not installed.\")\n",
    "\n",
    "        try:\n",
    "            if tf_mod is not None:\n",
    "                print(\"Training LSTM (simple)...\")\n",
    "                from sklearn.preprocessing import MinMaxScaler\n",
    "                from tensorflow.keras.models import Sequential\n",
    "                from tensorflow.keras.layers import LSTM, Dense\n",
    "                scaler = MinMaxScaler()\n",
    "                vals = s_df[target].astype('float32').values.reshape(-1,1)\n",
    "                scaled = scaler.fit_transform(vals)\n",
    "                def create_seq(data, n_steps=7):\n",
    "                    X, y = [], []\n",
    "                    for i in range(n_steps, len(data)):\n",
    "                        X.append(data[i-n_steps:i, 0])\n",
    "                        y.append(data[i, 0])\n",
    "                    return np.array(X), np.array(y)\n",
    "                n_steps = 7\n",
    "                X_all, y_all = create_seq(scaled, n_steps)\n",
    "                split_idx = int(len(X_all) * 0.8)\n",
    "                X_train, X_test = X_all[:split_idx], X_all[split_idx:]\n",
    "                y_train, y_test = y_all[:split_idx], y_all[split_idx:]\n",
    "                X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "                X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "                m = Sequential()\n",
    "                m.add(LSTM(32, input_shape=(n_steps, 1)))\n",
    "                m.add(Dense(1))\n",
    "                m.compile(optimizer='adam', loss='mse')\n",
    "                m.fit(X_train, y_train, epochs=20, batch_size=16, verbose=0)\n",
    "                preds_scaled = m.predict(X_test).flatten()\n",
    "                preds = scaler.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "                y_test_orig = scaler.inverse_transform(y_test.reshape(-1,1)).flatten()\n",
    "                l_mae, l_rmse = mae(y_test_orig, preds), rmse(y_test_orig, preds)\n",
    "                l_path = os.path.join(OUT_DIR, f\"lstm_{gname}.h5\")\n",
    "                m.save(l_path)\n",
    "                # save scaler\n",
    "                if _has_joblib:\n",
    "                    joblib.dump(scaler, os.path.join(OUT_DIR, f\"lstm_scaler_{gname}.pkl\"))\n",
    "                model_scores[\"LSTM\"] = (l_mae, l_rmse, l_path)\n",
    "                print(\"LSTM done:\", l_mae, l_rmse)\n",
    "            else:\n",
    "                print(\"LSTM skipped.\")\n",
    "        except Exception as e:\n",
    "            print(\"LSTM error:\", e)\n",
    "\n",
    "        # choose best by MAE then RMSE\n",
    "        if model_scores:\n",
    "            best_name = sorted(model_scores.items(), key=lambda x: (x[1][0], x[1][1]))[0][0]\n",
    "            best_mae, best_rmse, best_path = model_scores[best_name]\n",
    "            print(f\"Best model for group {gname} => {best_name} (MAE={best_mae:.4f}, RMSE={best_rmse:.4f}) path={best_path}\")\n",
    "            results.append({\"group\": gname, \"best_model\": best_name, \"mae\": best_mae, \"rmse\": best_rmse, \"path\": best_path})\n",
    "        else:\n",
    "            print(f\"No models trained for group {gname}.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error processing group\", grp, traceback.format_exc())\n",
    "\n",
    "# Save summary\n",
    "# -------- Save best models summary (CSV) --------\n",
    "if results:\n",
    "    res_df = pd.DataFrame(results)\n",
    "    res_df.to_csv(BEST_MODELS_PATH, index=False)  # ✅ uses correct variable\n",
    "    print(f\"\\n✅ Saved best model summary → {BEST_MODELS_PATH}\")\n",
    "else:\n",
    "    print(\"⚠️ No models trained.\")\n",
    "\n",
    "print(\"\\nAll artifacts →\", os.listdir(OUT_DIR))\n",
    "print(\"✅ Pipeline completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98e82e-e613-420c-9473-8d6a9a0d526c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
